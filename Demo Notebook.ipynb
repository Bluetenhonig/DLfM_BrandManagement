{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook to train, save, and load a model, which will be used on the Flask Webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('playground/data/mobile-price-classification/train.csv')\n",
    "\n",
    "#Changing pandas dataframe to numpy array\n",
    "X = dataset.iloc[:,:20].values\n",
    "y = dataset.iloc[:,20:21].values\n",
    "\n",
    "#Normalizing the data\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonym\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1800/1800 [==============================] - 2s 989us/step - loss: 1.5038 - acc: 0.2706\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 1.4033 - acc: 0.2900\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 1.3488 - acc: 0.3278\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 1.3028 - acc: 0.3683\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 1.2558 - acc: 0.4128\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 1.2031 - acc: 0.4533\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 1.1452 - acc: 0.4956\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 0s 88us/step - loss: 1.0812 - acc: 0.5306\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 1.0117 - acc: 0.5778\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.9394 - acc: 0.6122\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.8695 - acc: 0.6456\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.8009 - acc: 0.6878\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 0s 103us/step - loss: 0.7392 - acc: 0.7211\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.6824 - acc: 0.7428\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 0s 125us/step - loss: 0.6312 - acc: 0.7761\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.5830 - acc: 0.7922\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.5406 - acc: 0.8128\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.4998 - acc: 0.8244\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.4639 - acc: 0.8444\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 0s 81us/step - loss: 0.4322 - acc: 0.8583\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 0s 93us/step - loss: 0.4036 - acc: 0.8678\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.3784 - acc: 0.8772\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 0s 93us/step - loss: 0.3543 - acc: 0.8894\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 0s 90us/step - loss: 0.3338 - acc: 0.8922\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.3126 - acc: 0.9061\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.2938 - acc: 0.9128\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 0s 90us/step - loss: 0.2778 - acc: 0.9139\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.2613 - acc: 0.9167\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 0s 93us/step - loss: 0.2485 - acc: 0.9250\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.2368 - acc: 0.924 - 0s 112us/step - loss: 0.2364 - acc: 0.9250\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.2257 - acc: 0.9300\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 0s 97us/step - loss: 0.2140 - acc: 0.9328\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 0s 106us/step - loss: 0.2049 - acc: 0.9372\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 0s 125us/step - loss: 0.1965 - acc: 0.9383\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.1875 - acc: 0.9456\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.1798 - acc: 0.9472\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.1728 - acc: 0.9528\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.1672 - acc: 0.9511\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.1606 - acc: 0.9556\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 0s 99us/step - loss: 0.1551 - acc: 0.9544\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 0s 110us/step - loss: 0.1495 - acc: 0.9567\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.1458 - acc: 0.9606\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.1402 - acc: 0.9589\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.1369 - acc: 0.9583\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.1328 - acc: 0.9633\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 0s 82us/step - loss: 0.1284 - acc: 0.9633\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 0s 82us/step - loss: 0.1243 - acc: 0.9661\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.1219 - acc: 0.9633\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 0s 81us/step - loss: 0.1194 - acc: 0.9650\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 0s 159us/step - loss: 0.1164 - acc: 0.9683\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 0s 145us/step - loss: 0.1113 - acc: 0.9717\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 0s 190us/step - loss: 0.1095 - acc: 0.9717\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 0s 179us/step - loss: 0.1064 - acc: 0.9722\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 0s 170us/step - loss: 0.1031 - acc: 0.9722\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 0s 165us/step - loss: 0.1014 - acc: 0.9744\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 0s 138us/step - loss: 0.0990 - acc: 0.9761\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 0s 148us/step - loss: 0.0974 - acc: 0.9761\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0964 - acc: 0.9744\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 0s 100us/step - loss: 0.0931 - acc: 0.9750\n",
      "Epoch 60/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.0901 - acc: 0.9772\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - 0s 88us/step - loss: 0.0887 - acc: 0.9783\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 0s 70us/step - loss: 0.0873 - acc: 0.9783\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 93us/step - loss: 0.0850 - acc: 0.9811\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 0s 160us/step - loss: 0.0844 - acc: 0.9761\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0814 - acc: 0.9822\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 0s 70us/step - loss: 0.0797 - acc: 0.9817\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0778 - acc: 0.9817\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 0s 68us/step - loss: 0.0783 - acc: 0.9794\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 0s 68us/step - loss: 0.0755 - acc: 0.9833\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 0s 65us/step - loss: 0.0740 - acc: 0.9833\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 0s 70us/step - loss: 0.0718 - acc: 0.9839\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 0s 73us/step - loss: 0.0697 - acc: 0.9850\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0684 - acc: 0.9867\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0677 - acc: 0.9872\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 0s 74us/step - loss: 0.0668 - acc: 0.9867\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0649 - acc: 0.9883\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 0s 68us/step - loss: 0.0639 - acc: 0.9889\n",
      "Epoch 78/100\n",
      "1800/1800 [==============================] - 0s 69us/step - loss: 0.0648 - acc: 0.9856\n",
      "Epoch 79/100\n",
      "1800/1800 [==============================] - 0s 68us/step - loss: 0.0617 - acc: 0.9867\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - 0s 74us/step - loss: 0.0605 - acc: 0.9889\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 0s 69us/step - loss: 0.0589 - acc: 0.9889\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0590 - acc: 0.9889\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 0s 82us/step - loss: 0.0578 - acc: 0.9889\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0587 - acc: 0.9883\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0585 - acc: 0.9894\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 0s 74us/step - loss: 0.0553 - acc: 0.9889\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0554 - acc: 0.9906\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0530 - acc: 0.9917\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0508 - acc: 0.9900\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0508 - acc: 0.9928\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 0s 72us/step - loss: 0.0508 - acc: 0.9928\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0494 - acc: 0.9933\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 0s 69us/step - loss: 0.0475 - acc: 0.9950\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 0s 73us/step - loss: 0.0475 - acc: 0.9928\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0458 - acc: 0.9933\n",
      "Epoch 96/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0455 - acc: 0.9950\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 0s 73us/step - loss: 0.0455 - acc: 0.9933\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0441 - acc: 0.9944\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0423 - acc: 0.9956\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0419 - acc: 0.9961\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 91.0\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "1800/1800 [==============================] - 0s 172us/step - loss: 0.0417 - acc: 0.9944 - val_loss: 0.2723 - val_acc: 0.9100\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0409 - acc: 0.9944 - val_loss: 0.2775 - val_acc: 0.9100\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0403 - acc: 0.9950 - val_loss: 0.2752 - val_acc: 0.9100\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0409 - acc: 0.9939 - val_loss: 0.2759 - val_acc: 0.9150\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0389 - acc: 0.9967 - val_loss: 0.2819 - val_acc: 0.9250\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0393 - acc: 0.9950 - val_loss: 0.2801 - val_acc: 0.9100\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.0376 - acc: 0.9950 - val_loss: 0.2860 - val_acc: 0.9200\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.0363 - acc: 0.9961 - val_loss: 0.2861 - val_acc: 0.9100\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0365 - acc: 0.9967 - val_loss: 0.2870 - val_acc: 0.9150\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0370 - acc: 0.9961 - val_loss: 0.2891 - val_acc: 0.9200\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 0s 92us/step - loss: 0.0364 - acc: 0.9950 - val_loss: 0.2949 - val_acc: 0.9150\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0346 - acc: 0.9956 - val_loss: 0.2937 - val_acc: 0.9100\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0343 - acc: 0.9961 - val_loss: 0.2994 - val_acc: 0.9250\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 0s 102us/step - loss: 0.0389 - acc: 0.9922 - val_loss: 0.3057 - val_acc: 0.9100\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0339 - acc: 0.9961 - val_loss: 0.2970 - val_acc: 0.9100\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.0328 - acc: 0.9961 - val_loss: 0.3063 - val_acc: 0.9100\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.0317 - acc: 0.9961 - val_loss: 0.3088 - val_acc: 0.9100\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 0s 81us/step - loss: 0.0313 - acc: 0.9967 - val_loss: 0.3069 - val_acc: 0.9200\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0315 - acc: 0.9956 - val_loss: 0.3047 - val_acc: 0.9100\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.0303 - acc: 0.9961 - val_loss: 0.3057 - val_acc: 0.9150\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0297 - acc: 0.9961 - val_loss: 0.3089 - val_acc: 0.9100\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0290 - acc: 0.9967 - val_loss: 0.3100 - val_acc: 0.9200\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 0s 101us/step - loss: 0.0291 - acc: 0.9967 - val_loss: 0.3091 - val_acc: 0.9150\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.0281 - acc: 0.9972 - val_loss: 0.3123 - val_acc: 0.9150\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0279 - acc: 0.9967 - val_loss: 0.3201 - val_acc: 0.9100\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 0s 88us/step - loss: 0.0275 - acc: 0.9972 - val_loss: 0.3124 - val_acc: 0.9200\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 0s 96us/step - loss: 0.0273 - acc: 0.9956 - val_loss: 0.3189 - val_acc: 0.9150\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0265 - acc: 0.9967 - val_loss: 0.3229 - val_acc: 0.9200\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.0262 - acc: 0.9967 - val_loss: 0.3191 - val_acc: 0.9200\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.0255 - acc: 0.9972 - val_loss: 0.3232 - val_acc: 0.9150\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.0251 - acc: 0.9961 - val_loss: 0.3213 - val_acc: 0.9200\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 0s 136us/step - loss: 0.0249 - acc: 0.9972 - val_loss: 0.3246 - val_acc: 0.9200\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 0s 156us/step - loss: 0.0251 - acc: 0.9967 - val_loss: 0.3288 - val_acc: 0.9200\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 0s 193us/step - loss: 0.0258 - acc: 0.9972 - val_loss: 0.3270 - val_acc: 0.9100\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 0s 243us/step - loss: 0.0270 - acc: 0.9961 - val_loss: 0.3367 - val_acc: 0.9200\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 0s 153us/step - loss: 0.0249 - acc: 0.9978 - val_loss: 0.3292 - val_acc: 0.9200\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 0s 152us/step - loss: 0.0254 - acc: 0.9950 - val_loss: 0.3325 - val_acc: 0.9150\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 0s 158us/step - loss: 0.0235 - acc: 0.9983 - val_loss: 0.3361 - val_acc: 0.9200\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 0s 128us/step - loss: 0.0226 - acc: 0.9978 - val_loss: 0.3374 - val_acc: 0.9200\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 0s 90us/step - loss: 0.0231 - acc: 0.9978 - val_loss: 0.3390 - val_acc: 0.9200\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 0s 112us/step - loss: 0.0218 - acc: 0.9983 - val_loss: 0.3402 - val_acc: 0.9100\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 0s 99us/step - loss: 0.0219 - acc: 0.9978 - val_loss: 0.3389 - val_acc: 0.9200\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 0s 118us/step - loss: 0.0209 - acc: 0.9989 - val_loss: 0.3442 - val_acc: 0.9200\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.0221 - acc: 0.9983 - val_loss: 0.3400 - val_acc: 0.9150\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 0s 273us/step - loss: 0.0224 - acc: 0.9994 - val_loss: 0.3570 - val_acc: 0.9200\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 0s 101us/step - loss: 0.0218 - acc: 0.9972 - val_loss: 0.3481 - val_acc: 0.9200\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 0s 137us/step - loss: 0.0209 - acc: 0.9972 - val_loss: 0.3430 - val_acc: 0.9200\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 0s 94us/step - loss: 0.0197 - acc: 0.9989 - val_loss: 0.3469 - val_acc: 0.9200\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.0192 - acc: 0.9989 - val_loss: 0.3506 - val_acc: 0.9200\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0192 - acc: 0.9994 - val_loss: 0.3573 - val_acc: 0.9200\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 0s 119us/step - loss: 0.0192 - acc: 0.9972 - val_loss: 0.3515 - val_acc: 0.9200\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.0188 - acc: 0.9994 - val_loss: 0.3452 - val_acc: 0.9200\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 0s 98us/step - loss: 0.0190 - acc: 0.9989 - val_loss: 0.3580 - val_acc: 0.9200\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 0s 79us/step - loss: 0.0185 - acc: 0.9989 - val_loss: 0.3527 - val_acc: 0.9200\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 0s 107us/step - loss: 0.0177 - acc: 0.9989 - val_loss: 0.3530 - val_acc: 0.9200\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 0s 91us/step - loss: 0.0174 - acc: 0.9989 - val_loss: 0.3633 - val_acc: 0.9200\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0175 - acc: 0.9989 - val_loss: 0.3668 - val_acc: 0.9200\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 0s 90us/step - loss: 0.0169 - acc: 0.9994 - val_loss: 0.3599 - val_acc: 0.9200\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.3640 - val_acc: 0.9150\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.0162 - acc: 0.9994 - val_loss: 0.3678 - val_acc: 0.9200\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.998 - 0s 76us/step - loss: 0.0167 - acc: 0.9989 - val_loss: 0.3684 - val_acc: 0.9200\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.3712 - val_acc: 0.9200\n",
      "Epoch 63/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.3714 - val_acc: 0.9200\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 0s 73us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.3728 - val_acc: 0.9200\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0152 - acc: 0.9994 - val_loss: 0.3714 - val_acc: 0.9200\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.3731 - val_acc: 0.9200\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 0s 72us/step - loss: 0.0148 - acc: 0.9994 - val_loss: 0.3735 - val_acc: 0.9200\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.3790 - val_acc: 0.9200\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.3756 - val_acc: 0.9200\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.0141 - acc: 0.9994 - val_loss: 0.3812 - val_acc: 0.9200\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.3810 - val_acc: 0.9200\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.3815 - val_acc: 0.9200\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0132 - acc: 0.9994 - val_loss: 0.3846 - val_acc: 0.9200\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 0s 82us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.3832 - val_acc: 0.9200\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.3845 - val_acc: 0.9200\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.3905 - val_acc: 0.9200\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.3893 - val_acc: 0.9200\n",
      "Epoch 78/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.3860 - val_acc: 0.9200\n",
      "Epoch 79/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.3892 - val_acc: 0.9200\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.3880 - val_acc: 0.9200\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 0s 71us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.3938 - val_acc: 0.9200\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.3975 - val_acc: 0.9200\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0119 - acc: 0.9994 - val_loss: 0.3981 - val_acc: 0.9200\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.3958 - val_acc: 0.9150\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 0s 90us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.4018 - val_acc: 0.9200\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 0s 70us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.3986 - val_acc: 0.9150\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 0s 88us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.3961 - val_acc: 0.9200\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 0s 85us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 0.9150\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 0s 62us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.3990 - val_acc: 0.9150\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0114 - acc: 0.9994 - val_loss: 0.4035 - val_acc: 0.9200\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 0s 80us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.4036 - val_acc: 0.9200\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 0s 83us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.4071 - val_acc: 0.9150\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 0s 77us/step - loss: 0.0127 - acc: 0.9983 - val_loss: 0.4081 - val_acc: 0.9200\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 0s 92us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.4143 - val_acc: 0.9200\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 0s 81us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.9150\n",
      "Epoch 96/100\n",
      "1800/1800 [==============================] - 0s 81us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.4098 - val_acc: 0.9150\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 0s 88us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.4141 - val_acc: 0.9150\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.4179 - val_acc: 0.9150\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 0s 89us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.4172 - val_acc: 0.9150\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 0s 86us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.4154 - val_acc: 0.9150\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "#Converting predictions to label\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))\n",
    "    \n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)\n",
    "\n",
    "history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8c718762d283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./playground/demo_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Data Preprocessing have to be executed, then the loaded model can be used for the Testing and Visualisation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./playground/demo_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
